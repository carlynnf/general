# Histograms, Kernel Density Estimation, and Hypothesis Tests for Comparing Distributions

This activity will focus on displaying and comparing distributions. We'll also use an efficient algorithm for nearest neighbor searches. The relevant textbook sections are: 4.8.1 and 5.7.2 on histograms and the Scott/Freedman-Diaconis/Knuth/Bayesian blocks rules for bin size, 6.1.1 on Kernel Density Estimation (KDE), 4.7.2 on the Kolmogorov-Smirnov test and alternatives, and 2.5.2 on nearest neighbor searches.

1. Read in "ECO_DR1_withradec.csv" from this repo and extract the variables name, radeg, decdeg, grpcz, and cz. Define X, Y, Z coordinates, remembering that you must multiply radeg by cos(decdeg x pi/180.) to get equal size degrees. You should assume a Hubble constant of 70 km/s/Mpc to get Z from cz and grpcz, defining parallel sets of variables for the two redshift inputs. (For example, all the ones based on grpcz could have "nopec" appended to indicate that peculiar velocities have been removed.)

2. Find nearest neighbors with both sets of coordinates using cKDTree. Note that you must use k=2 with the .query function described on page 60 of the Ivezic et al. text because the first neighbor is considered to be the object itself at distance zero. Note the example of vectorization for speedup of nearest neighbor codes on pp. 54-56, and the subsequent commentary that vectorization is *not enough* to make these codes efficient -- clever "tree" algorithms are essential.

3. Plot histograms of your nearest neighbor distances (not the zero distances for the object itself) with the three main types of fixed bin size optimization: Scott, Freedman-Diaconis, and Knuth. Comment on how Scott does in each of your figures compared to Fig. 5.20.

4. Use the "blocks" option in astroML's version of hist to overplot variable bin size histograms. Note the spikes -- what causes these? How could you fix them in principle? (Don't try it now.)

5. Overplot unbinned distributions using KDE. The astroML version of KDE has been superseded by a new version in scikit-learn -- see [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html). Experiment with different kernels and bandwidths. Ivezic et al. identify the optimal kernel (which one?) but say that the bandwidth choice is much more important. It seems the bandwidth has taken the place of the bin width as the mysterious quantity we must somehow optimize. The book suggests cross-validation as a way to find the best bandwidth and we'll look at cross-validation more later, but for now we'll just say it involves subdividing the sample to calibrate the result with different subsamples.

6. In general, subdividing the sample is always a great idea for testing distributions. For example, there appears to be an exclusion zone in the "nopec" neighbor distances KDE plot for distances < 0.06 Mpc. To see if this exclusion zone is real, let's see if it shows up for two separate regions of the sky. Divide your sample into north and south at roughly Dec = 15 degrees, and plot the "nopec" neighbor distance distributions for the two subsamples in a new figure. Compare the distributions using the Kolmogorov-Smirnov (K-S) and Mann-Whitney (M-W) tests and print the probabilities that the two samples were drawn from the same parent sample on the figure. Is the exclusion zone real? Are the two subsamples fair to compare?

7. In preparation for our next activity on cross-validation, come up with a way to subdivide the sample that uses random index sampling to achieve fair, equal-size comparison subsamples. Because random samples can be unfair, run your code a few times to find a random division of the sample that ensures K-S and M-W tests don't see any significant difference. You can store this random subdivision

